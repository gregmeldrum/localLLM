{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyOhZLANjCOPlifOrOJk5OUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregmeldrum/localLLM/blob/main/free-tier-replacement-for-openai/GPT3_5_Replacement_Server_using_Intel_NeuralChat_7B_v3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cA8egayXpS-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a drop in replacement for GPT-3.5\n",
        "\n",
        "This colab starts an instance of the [Intel Neural Chat 7B v3.1](https://huggingface.co/Intel/neural-chat-7b-v3-1) model and makes the model publicly available with ngrok. You can use the model with the OpenAI API by setting the `openai.base_url` to your ngrok public url. Look for the ngrok public url at the end of the output for step 2. You can sign up to [ngrok](https://ngrok.com/) for free if you don't already have an account.\n",
        "\n",
        "The first inference will take about a minute since the model needs to be loaded into GPU. After that, each inference will take a few seconds.\n",
        "\n",
        "The Neural Chat 7B v3.1 model tops the Open LLM Leaderboard for it's model size at the time of writing this colab.\n",
        "\n",
        "This colab uses [ollama](https://ollama.ai/) to run the model, [litellm](https://litellm.ai/) to wrap it in an OpenAI API and [ngrok](https://ngrok.com/) to give it a public URL."
      ],
      "metadata": {
        "id": "wadBfu2Eo9DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b>Step 1 - Enter your ngrok authtoken from https://dashboard.ngrok.com/get-started/your-authtoken</b> and <b>run this cell</b>\n",
        "authtoken = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "O-_fc63fn4_H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Run the model**\n",
        "Look for the ngrok URL at the end of the output"
      ],
      "metadata": {
        "id": "j0ud2RUUzExL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set LD_LIBRARY_PATH so the system NVIDIA library becomes preferred\n",
        "# over the built-in library. This is particularly important for\n",
        "# Google Colab which installs older drivers\n",
        "import os\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "import os\n",
        "os.environ[\"AUTHTOKEN\"] = authtoken\n",
        "\n",
        "!pip install pyngrok==7.0.1 fastapi==0.104.1 uvicorn==0.24.0.post1\n",
        "\n",
        "# install ollama\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "# Start the ollama server (in the background)\n",
        "!nohup bash -c \"ollama serve\" &\n",
        "\n",
        "# Download the model from hugging face (6 bit quantized version - by default ollama uses 4 bit)\n",
        "!wget https://huggingface.co/TheBloke/neural-chat-7B-v3-1-GGUF/resolve/main/neural-chat-7b-v3-1.Q6_K.gguf?download=true -O /content/neural-chat-7b-v3-1.Q6_K.gguf\n",
        "\n",
        "# Create a model definition file where we tell Ollama the prompt template, temperature, stop tokens and anything\n",
        "# else we need for our usecase\n",
        "# (https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md)\n",
        "openchat_model_definition = '''\n",
        "from neural-chat-7b-v3-1.Q6_K.gguf\n",
        "\n",
        "TEMPLATE \"\"\"\n",
        "### System:\n",
        "{{- if .First }}\n",
        "{{ .System }}\n",
        "{{- end }}\n",
        "\n",
        "### User:\n",
        "{{ .Prompt }}\n",
        "\n",
        "### Assistant:\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM \"\"\"You are a helpful and friendly assistant. If you don't know the answer to the users question, just say you don't know.\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.3\n",
        "'''\n",
        "\n",
        "with open(\"/content/modelfile.neuralchat\", \"w\") as text_file:\n",
        "    text_file.write(openchat_model_definition)\n",
        "\n",
        "# Create a model in ollama for the 6bit quantized openchat\n",
        "!ollama create neuralchat -f /content/modelfile.neuralchat\n",
        "\n",
        "# Install and start litellm\n",
        "!pip install litellm\n",
        "!nohup bash -c \"litellm --host 127.0.0.1 --model ollama/neuralchat\" &\n",
        "\n",
        "# Start ngrok\n",
        "!ngrok config add-authtoken $AUTHTOKEN\n",
        "!ngrok http --log stderr 8000"
      ],
      "metadata": {
        "id": "D0dGrJ4sqIO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}