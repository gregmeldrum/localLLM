{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNRHUt1hZF5uYpBKBfIqOO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregmeldrum/localLLM/blob/main/free-tier-replacement-for-openai/GPT3_5_Replacement_Server_using_Openchat_3_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cA8egayXpS-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a drop in replacement for GPT-3.5\n",
        "\n",
        "This colab starts an instance of the [OpenChat-3.5](https://huggingface.co/openchat/openchat_3.5) model and makes the model publicly available with ngrok. You can use the model with the OpenAI API by setting the `openai.base_url` to your ngrok public url. Look for the ngrok public url at the end of the output for step 2. You can sign up to [ngrok](https://ngrok.com/) for free if you don't already have an account.\n",
        "\n",
        "The first inference will take about a minute since the model needs to be loaded into GPU. After that, each inference will take a few seconds.\n",
        "\n",
        "The OpenChat-3.5 model claims to be on par with OpenAI GPT 3.5 from March. GPT 3.5 has improved since then, but this model may still be good enough for your usecase.\n",
        "\n",
        "This colab uses [ollama](https://ollama.ai/) to run the model, [litellm](https://litellm.ai/) to wrap it in an OpenAI API and [ngrok](https://ngrok.com/) to give it a public URL."
      ],
      "metadata": {
        "id": "wadBfu2Eo9DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b>Step 1 - Enter your ngrok authtoken from https://dashboard.ngrok.com/get-started/your-authtoken</b> and <b>run this cell</b>\n",
        "authtoken = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "O-_fc63fn4_H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Run the model**\n",
        "Look for the ngrok URL at the end of the output"
      ],
      "metadata": {
        "id": "j0ud2RUUzExL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set LD_LIBRARY_PATH so the system NVIDIA library becomes preferred\n",
        "# over the built-in library. This is particularly important for\n",
        "# Google Colab which installs older drivers\n",
        "import os\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "import os\n",
        "os.environ[\"AUTHTOKEN\"] = authtoken\n",
        "\n",
        "!pip install pyngrok==7.0.1 fastapi==0.104.1 uvicorn==0.24.0.post1\n",
        "\n",
        "# install ollama\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "# Start the ollama server (in the background)\n",
        "!nohup bash -c \"ollama serve\" &\n",
        "\n",
        "# Download the model from hugging face (6 bit quantized version - by default ollama uses 4 bit)\n",
        "!wget https://huggingface.co/TheBloke/openchat_3.5-GGUF/resolve/main/openchat_3.5.Q6_K.gguf?download=true -O /content/openchat_3.5.Q6_K.gguf\n",
        "\n",
        "# Create a model definition file where we tell Ollama the prompt template, temperature, stop tokens and anything\n",
        "# else we need for our usecase\n",
        "# (https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md)\n",
        "openchat_model_definition = '''\n",
        "from openchat_3.5.Q6_K.gguf\n",
        "\n",
        "TEMPLATE \"\"\"\n",
        "{{- if .First }}\n",
        "System: {{ .System }}\n",
        "{{- end }}\n",
        "GPT4 User: {{ .Prompt }}<|end_of_turn|>GPT4 Assistant:\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM \"\"\"You are a helpful and friendly assistant. If you don't know the answer to the users question, just say you don't know.\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.3\n",
        "PARAMETER stop \"<|end_of_turn|>\"\n",
        "PARAMETER stop \"GPT4 User:\"\n",
        "PARAMETER stop \"GPT4:\"\n",
        "'''\n",
        "\n",
        "with open(\"/content/Modelfile.openchat-truth\", \"w\") as text_file:\n",
        "    text_file.write(openchat_model_definition)\n",
        "\n",
        "# Create a model in ollama for the 6bit quantized openchat\n",
        "!ollama create openchat-truthful -f /content/Modelfile.openchat-truth\n",
        "\n",
        "# Install and start litellm\n",
        "!pip install litellm\n",
        "!nohup bash -c \"litellm --host 127.0.0.1 --model ollama/openchat-truthful\" &\n",
        "\n",
        "# Start ngrok\n",
        "!ngrok config add-authtoken $AUTHTOKEN\n",
        "!ngrok http --log stderr 8000"
      ],
      "metadata": {
        "id": "D0dGrJ4sqIO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}