{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyP5ZYKIDvNt/LsgfyVF2Uop",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregmeldrum/localLLM/blob/main/free-tier-replacement-for-copilot/Deepseek_Coder6_7B_Server_as_copilot_replacement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cA8egayXpS-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deepseek Coder 6.7B Server as Copilot Replacement\n",
        "\n",
        "This colab starts an instance of the [Deepseek Coder](https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct) model (6 bit quantized) and makes the model publicly available with ngrok. Look for the ngrok public url at the end of the output for step 2. You can sign up to [ngrok](https://ngrok.com/) for free if you don't already have an account.\n",
        "\n",
        "To verify that the server is running, check the OpenAPI spec at https://dd26-34-142-187-115.ngrok-free.app/docs (replace the ngrok URL with your own and add `/docs` at the end)\n",
        "\n",
        "In order to use this with github copilot on VSCode, open the settings for the github copilot plugin, and add the following to the advanced settings:\n",
        "```\n",
        "{\n",
        "    \"github.copilot.advanced\": {\n",
        "        \"debug.testOverrideProxyUrl\": \"https://dd26-34-142-187-115.ngrok-free.app\",\n",
        "        \"debug.overrideProxyUrl\": \"https://dd26-34-142-187-115.ngrok-free.app\"\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "Replace the ngrok url above with your own.\n",
        "\n",
        "This colab uses a 6bit quantized gguf version of the 6.7B instruct model.\n",
        "\n",
        "This colab uses [llama-cpp-server](https://github.com/abetlen/llama-cpp-python) to run the model and serve the copilot interface and [ngrok](https://ngrok.com/) to give it a public URL."
      ],
      "metadata": {
        "id": "wadBfu2Eo9DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b>Step 1 - Enter your ngrok authtoken from https://dashboard.ngrok.com/get-started/your-authtoken</b> and <b>run this cell</b>\n",
        "authtoken = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX' # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "O-_fc63fn4_H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Run the model**\n",
        "Look for the ngrok URL at the end of the output"
      ],
      "metadata": {
        "id": "j0ud2RUUzExL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set LD_LIBRARY_PATH so the system NVIDIA library becomes preferred\n",
        "# over the built-in library. This is particularly important for\n",
        "# Google Colab which installs older drivers\n",
        "import os\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "import os\n",
        "os.environ[\"AUTHTOKEN\"] = authtoken\n",
        "\n",
        "# Download the model from hugging face (6 bit quantized version - by default ollama uses 4 bit)\n",
        "!wget https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q6_K.gguf?download=true -O /content/deepseek-coder-6.7b-instruct.Q6_K.gguf\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python[server] --force-reinstall --upgrade --no-cache-dir\n",
        "!nohup bash -c \"python -m llama_cpp.server --n_ctx 4096 --n_gpu_layers 50 --model /content/deepseek-coder-6.7b-instruct.Q6_K.gguf --host 0.0.0.0\" &\n",
        "\n",
        "# Start ngrok\n",
        "!pip install pyngrok==7.0.1\n",
        "!ngrok config add-authtoken $AUTHTOKEN\n",
        "!ngrok http --log stderr 8000"
      ],
      "metadata": {
        "id": "D0dGrJ4sqIO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ER-2fpHV156X"
      }
    }
  ]
}