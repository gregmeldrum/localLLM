{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "NtYQ86RMwU2J",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running an Llama2 LLM in Google Colab.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/gregmeldrum/localLLM/blob/main/colab/Llama2_Orca_Mini_3.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This colab is used to run any of the Llama2 LLM and derivitives.\n",
        "\n",
        "Due to the resource constraints of colab, we'll use a 5 bit quantized 13B parameter model in GGML format. The [llama cpp](https://github.com/ggerganov/llama.cpp) projects supports GGML formatted models and allows the model to run on a combination of CPU and GPU.\n",
        "\n",
        "We'll use [langchain](https://python.langchain.com/) to run the model.\n",
        "\n",
        "The model used in this colab is baed on [Orca-Mini-v3-13B](https://huggingface.co/psmathur/orca_mini_v3_13b) which (at the time of writing this note) is one of the top rated 13B model on the [Hugging Face LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). More specifically, we'll be using the [GGML quantized version](https://huggingface.co/TheBloke/orca_mini_v3_13B-GGML) created by [The Bloke](https://huggingface.co/TheBloke).\n",
        "\n",
        "This model uses the following prompt template:\n",
        "\n",
        "```\n",
        "### System:\n",
        "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
        "\n",
        "### User:\n",
        "{prompt}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "```\n",
        "\n",
        "We won't be using `Input`, so we'll remove that from the template."
      ],
      "metadata": {
        "id": "uixrdD8DGwCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll install and compile the `llama-cpp-python` library and then download the ggml llm model from Hugging Face."
      ],
      "metadata": {
        "id": "kt5Pk8tyeCSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install -q llama-cpp-python --no-cache-dir\n",
        "\n",
        "!apt-get -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/orca_mini_v3_13B-GGML/resolve/main/orca_mini_v3_13b.ggmlv3.q5_K_M.bin -d /content/ -o orca_mini_v3_13b.ggmlv3.q5_K_M.bin\n"
      ],
      "metadata": {
        "id": "6BiwClR2vt3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll install langchain and create the llm and llm chain. For a description of the parameters used to configure the LlamaCpp LLM see the [API definition](https://api.python.langchain.com/en/latest/llms/langchain.llms.llamacpp.LlamaCpp.html#langchain.llms.llamacpp.LlamaCpp)."
      ],
      "metadata": {
        "id": "2wEDy5GBqggo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain"
      ],
      "metadata": {
        "id": "l3jYaD21uEDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "template = \"\"\"### System:\n",
        "You are an AI assistant that follows instruction extremely well. Help as much as you can.\n",
        "\n",
        "### User:\n",
        "{question}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager\n",
        "\n",
        "temperature = 0.75 # Use a value between 0 and 2. Lower = factual, higher = creative\n",
        "n_gpu_layers = 43  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 2048  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/orca_mini_v3_13b.ggmlv3.q5_K_M.bin\",\n",
        "    temperature=temperature,\n",
        "    max_tokens=2048,\n",
        "    n_ctx=2048,\n",
        "    top_p=0.95,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
      ],
      "metadata": {
        "id": "gHjJMJJxqkYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets test out the model."
      ],
      "metadata": {
        "id": "Fr_9wU0x1sfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Write the python code to calculate a least squares regression line for a list of x , y coordinates. Output the slope and intercept\"\n",
        "\n",
        "output = llm_chain.run(question)"
      ],
      "metadata": {
        "id": "1vdXFpmS21S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who was the prime minister of Canada on the day that Justin Beiber was born. Explain your reasoning.\"\n",
        "\n",
        "output = llm_chain.run(question)"
      ],
      "metadata": {
        "id": "YDHn2QlStQ_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who would win in a fight, Batman or Superman.\"\n",
        "\n",
        "output = llm_chain.run(question)\n"
      ],
      "metadata": {
        "id": "8GsBx5uSyNtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"If Taylor is faster than Rahul and Rahul is faster than Juan, is Taylor faster than Juan? Explain your reasoning.\"\n",
        "\n",
        "output = llm_chain.run(question)"
      ],
      "metadata": {
        "id": "ZSrUtAyYkKer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Write a short distopian story about how in the near future, AI will become the master of the human race.\"\n",
        "\n",
        "output = llm_chain.run(question)"
      ],
      "metadata": {
        "id": "0tCrK4KokxWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Is the meaning of life really 42?\"\n",
        "\n",
        "output = llm_chain.run(question)"
      ],
      "metadata": {
        "id": "OeZ_m6Qy5rNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "BBQ0PfTxlBmg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}